# 大模型数据集生成

## 1.Bonito

[代码](https://github.com/BatsResearch/bonito?tab=readme-ov-file)

[论文](https://arxiv.org/abs/2402.18334)

​		Bonito，一个用于条件任务生成的开源模型，它可以将未标注的文本转换为特定任务的训练数据集，用于指令微调。根据论文介绍，该模型本身是在 mistralai/Mistral-7B-v0.1 的基础上，利用包含 165 万个示例的数据集进行微调，支持多种任务类型，包括多选题回答、是非题回答、自然语言推理、主题分类等。

<img src="数据集生成.assets/image-20240825141744791.png" alt="image-20240825141744791" style="zoom:80%;" />

​		  Bonito采用未注释的文本作为输入以及任务属性，以生成指令调整数据。对于每个未注释的文本，它会生成引用该文本和目标响应的指令。然后，指令调整数据用于（进一步）微调语言模型，使其适应专业领域的任务。

<img src="数据集生成.assets/image-20240825142256908.png" alt="image-20240825142256908" style="zoom:67%;" />



### 构建带属性的条件任务生成（Conditional Task Generation with Attributes，CTGA）数据集的过程。

​		首先，从 P3（Public Pool of Prompts）中识别需要一段段落或上下文才能完成任务的数据集。确定了 CTGA 中总共包含 38 个数据集。对于每个数据集，从 P3 收集 Jinja1 模板。

​		接下来，重新混合 Jinja 模板以创建元模板。由于 P3 中的 Jinja 模板不包含任务类型，因此使用目标任务类型（例如是非问答）手动注释它们。

​		我们获得了涵盖 16 种任务类型的 323 个元模板（有关任务类型的列表，请参见表 13）。

最后，将元模板应用于数据集中的所有示例以创建 CTGA 数据集。

### 性能

![image-20240825143422806](数据集生成.assets/image-20240825143422806.png)

针对预训练好的模型，0样本任务适应性能，针对mistral和llama2，Bonito优于None和TAPT。

>Task-Adaptive PreTraining（TAPT）是指**在第一阶段通用预训练模型的基础上，利用任务相关未标注文本继续训练**

<img src="数据集生成.assets/image-20240825144443602.png" alt="image-20240825144443602" style="zoom:67%;" />

针对微调后的模型，0样本任务适应性能，针对mistral和llama2，Bonito优于None和TAPT。

## 2.基于预训练大模型的数据集生成

### 1.ChatGPT

[参考](https://blog.csdn.net/u012960155/article/details/132658756)

编写python脚本利用gpt3.5将文本文档转化为问答对。代码示例如下：

```python

"""
    目标：将大段文档通过gpt3.5识别变成一问一答的问答对。
    流程：1.gpt自动获取合适的问题；2.gpt自动根据问题和文档生成问答对。
    优点：几乎无需人工介入，自动获取问题，自动根据问题生成问答对。
    缺点：受限于大模型输入长度限制，可能无法一次性输入全部文档。
    建议：使用gpt3.5-16k可以一次输入大量文本，文档最好不超过5000字。

    FAQ：
    1.Q：gpt两个步骤是否可以合并成一个请求让gpt返回，可以节省约一半的时间和tokens？
      A：拆成两次主要是因为问题可能需要人工微调修改后再去生成答案，这样可以提高知识库质量，当然也可以全部自动处理。
    2.Q：大模型有字数限制无法大文档一次输入？
      A：目前这个没有好的解决办法，只能通过预先拆分大文档为多个文档片段后分批执行。
"""
import datetime
import time
import requests
url = 'https://api.openai.com/v1/chat/completions'
# 替换为您自己的API密钥
api_key = 'sk-xxxxxxxxx'
model = "gpt-3.5-turbo-16k"
prompt1 = '''
#01 你是一个问答对数据集处理专家。

#02 你的任务是根据我给出的内容，生成适合作为问答对数据集的问题。

#03 问题要尽量短，不要太长。

#04 一句话中只能有一个问题。

#05 生成的问题必须宏观、价值，不要生成特别细节的问题。

#06 生成问题示例：

"""

权益型基金的特点有哪些方面？

介绍一下产品经理。

"""

#07 以下是我给出的内容：

"""

{{此处替换成你的内容}}

"""
'''
prompt2 = '''
#01 你是一个问答对数据集处理专家。

#02 你的任务是根据我的问题和我给出的内容，生成对应的问答对。

#03 答案要全面，多使用我的信息，内容要更丰富。

#04 你必须根据我的问答对示例格式来生成：

"""

{"content": "基金分类有哪些", "summary": "根据不同标准，可以将证券投资基金划分为不同的种类：（1）根据基金单位是否可增加或赎回，可分为开放式基金和封闭式基金。开放式基金不上市交易（这要看情况），通过银行、券商、基金公司申购和赎回，基金规模不固定；封闭式基金有固定的存续期，一般在证券交易场所上市交易，投资者通过二级市场买卖基金单位。（2）根据组织形态的不同，可分为公司型基金和契约型基金。基金通过发行基金股份成立投资基金公司的形式设立，通常称为公司型基金；由基金管理人、基金托管人和投资人三方通过基金契约设立，通常称为契约型基金。我国的证券投资基金均为契约型基金。（3）根据投资风险与收益的不同，可分为成长型、收入型和平衡型基金。（4）根据投资对象的不同，可分为股票基金、债券基金、货币基金和混合型基金四大类。"}

{"content": "基金是什么", "summary": "基金，英文是fund，广义是指为了某种目的而设立的具有一定数量的资金。主要包括公积金、信托投资基金、保险基金、退休基金，各种基金会的基金。从会计角度透析，基金是一个狭义的概念，意指具有特定目的和用途的资金。我们提到的基金主要是指证券投资基金。"}

#05 我的问题如下：

"""

{{此处替换成你上一步生成的问题}}

"""

#06 我的内容如下：

"""

{{此处替换成你的内容}}

"""
'''
def generate_question(text_content, more=False):
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    content = "生成适合作为问答对的问题"
    if more:
        content = "尽可能多生成适合作为问答对的问题"
    prompt = prompt1.replace("{{此处替换成你的内容}}", text_content)
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": prompt},
            {"role": "user", "content": content}
        ]
    }
    start_time = time.time()
    response = requests.post(url, headers=headers, json=data, verify=False)
    print("耗时", time.time() - start_time)
    if response.status_code == 200:
        return response.json()["choices"][0]["message"]['content']
    else:
        print(f"Error: {response.status_code}")
        print(response.content)
        return None
def generate_qa(text_content, question_text=None):
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    prompt = prompt2.replace("{{此处替换成你上一步生成的问题}}", question_text).replace("{{此处替换成你的内容}}", text_content)
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "拼成问答对"}
        ]
    }
    start_time = time.time()
    response = requests.post(url, headers=headers, json=data, verify=False)
    print("耗时", time.time() - start_time)
    if response.status_code == 200:
        return response.json()["choices"][0]["message"]['content']
    else:
        print(f"Error: {response.status_code}")
        print(response.content)
        return None
def write_to_file(content):
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    file_name = f"new_file_{timestamp}.txt"
    with open(file_name, "w") as file:
        file.write(content)
    print("File 'new_file.txt' has been created and written.")
def read_file(file_name):
    try:
        with open(file_name, "r") as file:
            content = file.read()
        return content
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
def main():
    text_content = read_file("input_file.txt")
    print('text_content\n', text_content)
    question_text = generate_question(text_content=text_content, more=True)
    print('question_text\n', question_text)
    qa_text = generate_qa(text_content=text_content, question_text=question_text)
    print('qa_text\n', qa_text)
    write_to_file(qa_text)
main()

```

运行上述Python脚本，它将自动从文档中提取问题，并生成与之对应的答案（输出到 new_file_{timestamp}.txt 文件）。这个脚本也会在控制台上显示提取的问题和生成的答案。



### 2.文心一言

[参考](https://blog.csdn.net/phyllis0065/article/details/140146632)

与上文不同的是，此方法加入从pdf中提取txt文本，将txt文本分割，以满足文心一言prompt对字符数和token数的限制。

具体步骤如下：

1. 从PDF中提取文本
2. 按生成式模型能接受的最大字符数和token数分割文本；
3. 定义问题生成prompt
4. 定义问答对生成prompt
5. 定义问题生成函数
6. 定义问答对生成函数
7. 主程序：使用生成式模型生成问答对，并将结果写入txt文件；

```python
import datetime
import time
import fitz  # PyMuPDF
import requests
import json
import numpy as np
from transformers import BertTokenizer
 
 
# 从PDF中提取文本
# 从PDF文件中提取文本函数
def extract_text_from_pdf(pdf_path):
    text = ""
    pdf_document = fitz.open(pdf_path)
    for page_num in range(len(pdf_document)):
        page = pdf_document.load_page(page_num)
        text += page.get_text()
    return text
 
#定义文本分割函数
def split_text(text, max_length, max_tokens):
    """将文本按字符数和token数分割成不超过max_length字符和max_tokens的段落"""
    paragraphs = []
    current_paragraph = ""
    current_tokens = 0
 
    for line in text.split("\n"):
        line_tokens = tokenizer.encode(line, add_special_tokens=False)
        if (len(current_paragraph) + len(line) + 1 <= max_length) and (current_tokens + len(line_tokens) + 1 <= max_tokens):
            current_paragraph += line + "\n"
            current_tokens += len(line_tokens) + 1  # +1 for the newline token
        else:
            paragraphs.append(current_paragraph.strip())
            current_paragraph = line + "\n"
            current_tokens = len(line_tokens) + 1
 
    if current_paragraph:
        paragraphs.append(current_paragraph.strip())
    return paragraphs
    
# 初始化BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
 
# 提取PDF文件中的文本并按token数和字符数分割
pdf_files = ["CSST科学白皮书_v1.2.pdf"]
max_length = 18000
max_tokens = 4620
documents = []
for pdf in pdf_files:
    text = extract_text_from_pdf(pdf)
    paragraphs = split_text(text, max_length, max_tokens)
    documents.extend(paragraphs)
 
# 将分割后的文本块分别存储到多个txt文件中，并保存文件名到一个列表中
file_names = []
for idx, doc in enumerate(documents, start=1):
    file_name = f"text_file{idx}.txt"
    with open(file_name, "w", encoding="utf-8") as file:
        file.write(doc)
    file_names.append(file_name)
 
print("文本文件已成功生成。")
print("生成的文件名列表：", file_names)
 
#定义问题生成prompt
prompt1 = '''
#01 你是一个问答对数据集处理专家。
#02 你的任务是根据我给出的内容，生成适合作为问答对数据集的问题。
#03 问题要尽量短，不要太长。
#04 一句话中只能有一个问题。
#05 最多生成15个问题。
#06 生成问题示例：
"""
"积分视场光谱仪是什么？"
"多通道成像仪的研制单位是哪个？"
介绍一下暗能量。
"""
#07 以下是我给出的内容：
"""
{{此处替换成你的内容}}
"""
'''
 
#定义问答对生成prompt
prompt2 = '''
#01 你是一个问答对数据集处理专家。
#02 你的任务是根据我的问题和我给出的内容，生成对应的问答对。
#03 答案要全面，只使用我的信息，如果找不到答案，就回复从文档中找不到答案。
#04 你必须根据我的问答对示例格式来生成：
"""
{"content": "星冕仪模块三个主要观测目标是什么？", "summary": "星冕仪模块是三个主要观测目标是：1.近邻恒星高对比度成像普查。2.视向速度探测已知系外行星后随观测。3.恒星星周盘高对比度成像监测，并对恒星外星黄道尘强度分布进行定量分析。"}
{"content": "空间站光学仓是什么？", "summary": "中国空间站光学舱将是一台 2 米口径的空间天文望远镜，主要工作在近紫外-可见光-近红外波段，兼具大视场巡天和精细观测能力，立足于2020-30 年代国际天文学研究的战略前沿，在科学上具有极强的竞争力，将与欧美同期的大型天文项目并驾齐驱，优势互补，并在若干方向上有所超越，有望取得对宇宙认知的重大突破"}
#05 我的问题如下：
"""
{{此处替换成你上一步生成的问题}}
"""
#06 我的内容如下：
"""
{{此处替换成你的内容}}
"""
'''
 
#配置文心一言
# 设置百度文心一言的API密钥和端点
API_KEY = "your_api_key"
SECRET_KEY = "your_secret_key"
 
def get_access_token():
    """
    使用 AK，SK 生成鉴权签名（Access Token）
    :return: access_token，或是None(如果错误)
    """
    url = "https://aip.baidubce.com/oauth/2.0/token"
    params = {"grant_type": "client_credentials", "client_id": API_KEY, "client_secret": SECRET_KEY}
    return str(requests.post(url, params=params).json().get("access_token"))
 
 
#定义问题生成函数
def generate_question(text_content, more=False):
    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions?access_token=" + get_access_token()
    content= "生成适合作为问答对的问题"
    if more:
        content = "尽可能多生成适合作为问答对的问题"
    prompt = prompt1.replace("{{此处替换成你的内容}}", text_content)
    payload = json.dumps({
        "messages": [
            {
                "role": "user",
                "content": content
            }
        ],
        "temperature": 0.95,
        "top_p": 0.8,
        "system":prompt
    })
    headers = {
        'Content-Type': 'application/json'
    }
    start_time = time.time()
    response = requests.request("POST", url, headers=headers, data=payload)
    x = json.loads(response.text)
    print("耗时", time.time() - start_time)
    print(x)
    if response.status_code == 200:
        return x['result']
    else:
        print(f"Error: {response.status_code}")
        print(response.content)
        return None
 
 
#定义问答对生成函数
def generate_qa(text_content, question_text=None):
    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions?access_token=" + get_access_token()
    content= "拼成问答对"
    prompt = prompt2.replace("{{此处替换成你上一步生成的问题}}", question_text).replace("{{此处替换成你的内容}}", text_content)
    payload = json.dumps({
        "messages": [
            {
                "role": "user",
                "content": content
            }
        ],
        "temperature": 0.95,
        "top_p": 0.8,
        "system":prompt
    })
    headers = {
        'Content-Type': 'application/json'
    }
    start_time = time.time()
    response = requests.request("POST", url, headers=headers, data=payload)
    x = json.loads(response.text)
    print("耗时", time.time() - start_time)
    print(x)
    if response.status_code == 200:
        return x['result']
    else:
        print(f"Error: {response.status_code}")
        print(response.content)
        return None
 
 
#将生成的问答对写入.txt文件
def write_to_file(content):
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    file_name = f"new_file_{timestamp}.txt"
    with open(file_name, "w", encoding="utf-8") as file:
        file.write(content)
    print("File 'new_file.txt' has been created and written.")
 
 
#读取PDF生成的txt文件
def read_file(file_name):
    try:
        with open(file_name, "r", encoding='utf-8') as file:
            content = file.read()
        return content
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
 
 
#主程序
def main():
    for file in file_names:
        text_content = read_file(file)
        print ('text_content\n', text_content)
        question_text = generate_question(text_content=text_content, more=False)
        print('question_text\n', question_text)
        qa_text = generate_qa(text_content=text_content, question_text=question_text)
        print('qa_text\n', qa_text)
        write_to_file(qa_text)
 
if __name__ == '__main__':
    main()
```

## Kimi+Langchain

[原文](https://blog.csdn.net/xx_nm98/article/details/141269778?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-141269778-blog-132658756.235^v43^pc_blog_bottom_relevance_base1&spm=1001.2101.3001.4242.1&utm_relevant_index=1)



























